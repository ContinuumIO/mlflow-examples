{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Training Demo\n",
    "\n",
    "Creates a model for predicting the quality of wine using [sklearn.linear_model.ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html).  We perform a naive search of the hyperparameter space in order to determine the optimal values.\n",
    "\n",
    "The results of the model training runs are tracked in an MLflow experiment. The best performing model is then registered in the model registry and set to the `Production` stage for usage.\n",
    "\n",
    "> This is notebook is based on `train.py` from the MLflow example [sklearn_elasticnet_wine](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine).\n",
    "\n",
    "Attribution\n",
    "* The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality and sourced from [here](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/sklearn_elasticnet_wine/wine-quality.csv).\n",
    "* P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "* Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Setup\n",
    "\n",
    "Create our experiment to track all our model training runs in.\n",
    "\n",
    "* This experiment is used across runs of the notebook and will not be recreated if it already exists.\n",
    "* The name of the experiment is defined as an anaconda project variable located within `anaconda-project.yml`.\n",
    "    * The variable name is `MLFLOW_EXPERIMENT_NAME`, and the default value is `demo_sklearn_elasticnet_wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from environment_utils import init\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "experiment_id, client = init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training performance evaluation function\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_utils import DataSet\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "\"\"\"\n",
    "Model Training Function\n",
    "Note that the hyper-parameters `alpha` and `l1_ratio` are parameters.  This parameterization allows us to drive a hyper-parameter search for optimizing model performance.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from mlflow_adsp import create_unique_name\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def train(alpha: float, l1_ratio: float, ds: DataSet) -> str:\n",
    "    # Start the MLflow run to track the model training.\n",
    "    with mlflow.start_run(run_name=create_unique_name(name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"])) as run:\n",
    "        # Create the model\n",
    "        lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "\n",
    "        # Fit the model\n",
    "        lr.fit(ds.X_train, ds.y_train)\n",
    "\n",
    "        # Assess model performance\n",
    "        predicted_qualities = lr.predict(ds.X_test)\n",
    "        (rmse, mae, r2) = eval_metrics(ds.y_test, predicted_qualities)\n",
    "\n",
    "        # Log our training hyper-parameters\n",
    "        mlflow.log_param(\"alpha\", alpha)\n",
    "        mlflow.log_param(\"l1_ratio\", l1_ratio)\n",
    "\n",
    "        # Log our model performance metrics.\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "        # Generate our model signatures for consumption.\n",
    "        predictions = lr.predict(ds.X_train)\n",
    "        signature = infer_signature(ds.X_train, predictions)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(lr, \"model\", signature=signature)\n",
    "\n",
    "        # Return the run_id for training run comparisons.\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_utils import prepare_data\n",
    "from mlflow.entities import Run\n",
    "\n",
    "alpha: float = 0.2\n",
    "l1_ratio: float = 0.1\n",
    "\n",
    "DATA_SET_FILENAME: str = \"datasets/winequality-white.csv\"\n",
    "data_set: DataSet = prepare_data(csv_url=DATA_SET_FILENAME)\n",
    "\n",
    "run_id: str = train(alpha=alpha, l1_ratio=l1_ratio, ds=data_set)\n",
    "run: Run = client.search_runs([experiment_id], f\"attributes.run_id = '{run_id}'\")[0]\n",
    "\n",
    "print(run.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a naive search of the hyperparameter space\n",
    "\n",
    "We will naively review model performance at specific internals across the solution space.  There are many optimization functions, which can be leveraged based on business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "runs: list[str] = []\n",
    "\n",
    "for i in trange(5):\n",
    "    alpha: float = i * 0.1\n",
    "    for j in trange(5, leave=False):\n",
    "        l1_ratio: float = j * 0.1\n",
    "        run_id: str = train(\n",
    "            alpha=alpha,\n",
    "            l1_ratio=l1_ratio,\n",
    "            ds=prepare_data(csv_url=DATA_SET_FILENAME),\n",
    "        )\n",
    "        runs.append(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and register the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Run\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def get_best_run(client: MlflowClient, experiment_id, runs: list[str]) -> tuple[Optional[Run], dict]:\n",
    "    _inf = np.finfo(np.float64).max\n",
    "\n",
    "    best_metrics: dict = {\n",
    "        \"rmse\": _inf,\n",
    "        \"mae\": _inf,\n",
    "        \"r2\": _inf,\n",
    "    }\n",
    "    best_run: Optional[Run] = None\n",
    "\n",
    "    for run_id in runs:\n",
    "        # find the best run, log its metrics as the final metrics of this run.\n",
    "        run: Run = client.search_runs([experiment_id], f\"attributes.run_id = '{run_id}'\")[0]\n",
    "\n",
    "        if run.data.metrics[\"rmse\"] < best_metrics[\"rmse\"]:\n",
    "            best_metrics = run.data.metrics\n",
    "            best_run = run\n",
    "\n",
    "    return best_run, best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the runs for the best performing model and add it to the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow_utils import register_best_model\n",
    "from mlflow.entities.model_registry import ModelVersion\n",
    "\n",
    "(run, metrics) = get_best_run(client=client, experiment_id=experiment_id, runs=runs)\n",
    "print(f\"Run ID: {run.info.run_id}\")\n",
    "print(f\"Report: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version: ModelVersion = register_best_model(client=client, run=run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promote the latest model to the `Production` stage for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version: ModelVersion = client.transition_model_version_stage(\n",
    "    name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
    "    version=model_version.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
