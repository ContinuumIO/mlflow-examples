{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Training Demo\n",
    "\n",
    "Creates a model for predicting the quality of wine using [xgboost.XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html).  We perform a naive search of the hyperparameter space in order to determine the optimal values.\n",
    "\n",
    "The results of the model training runs are tracked in an MLflow experiment. The best performing model is then registered in the model registry and set to the `Production` stage for usage.\n",
    "\n",
    "> This is notebook is based on `train.py` from the MLflow example [xgboost_sklearn](https://github.com/mlflow/mlflow/tree/master/examples/xgboost/xgboost_sklearn).\n",
    "\n",
    "Attribution\n",
    "* The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality and sourced from [here](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/sklearn_elasticnet_wine/wine-quality.csv).\n",
    "* P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "* Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:53.421761Z",
     "start_time": "2023-08-07T20:46:52.110578Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "\n",
    "from mlflow_adsp import create_unique_name, upsert_experiment\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:54.649274Z",
     "start_time": "2023-08-07T20:46:54.644372Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load user specific configuration.\n",
    "\"\"\"\n",
    "\n",
    "from ae5_tools import load_ae5_user_secrets\n",
    "\n",
    "load_ae5_user_secrets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Setup\n",
    "\n",
    "Create our experiment to track all our model training runs in.\n",
    "\n",
    "* This experiment is used across runs of the notebook and will not be recreated if it already exists.\n",
    "* The name of the experiment is defined as an anaconda project variable located within `anaconda-project.yml`.\n",
    "    * The variable name is `MLFLOW_EXPERIMENT_NAME`, and the default value is `demo_sklearn_elasticnet_wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:57.079076Z",
     "start_time": "2023-08-07T20:46:57.075674Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Generate a client, this will be used for several operations across the notebook.\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:57.834181Z",
     "start_time": "2023-08-07T20:46:57.828196Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "\n",
    "def upsert_model_registry(client: MlflowClient) -> None:\n",
    "    try:\n",
    "        client.create_registered_model(name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"])\n",
    "    except MlflowException as error:\n",
    "        if error.error_code != \"RESOURCE_ALREADY_EXISTS\":\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:46:58.539775Z",
     "start_time": "2023-08-07T20:46:58.293590Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure that the experiment and model registry exist for reporting and tracking.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from mlflow.entities import Experiment\n",
    "\n",
    "experiment: Experiment = mlflow.set_experiment(experiment_id=upsert_experiment())\n",
    "upsert_model_registry(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:47:04.790223Z",
     "start_time": "2023-08-07T20:47:04.776407Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preparation\n",
    "Loads the data from csv file, and returns our train, test splits for training.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class DataSet(BaseModel):\n",
    "    X_train: pd.DataFrame\n",
    "    X_test: pd.DataFrame\n",
    "    y_train: pd.DataFrame\n",
    "    y_test: pd.DataFrame\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "def prepare_data(csv_url: str) -> DataSet:\n",
    "    data: pd.DataFrame = pd.read_csv(csv_url, sep=\",\")\n",
    "\n",
    "    # The predicted column is `quality`, which is a scalar from [3, 9]\n",
    "    X: pd.DataFrame = data.drop([\"quality\"], axis=1)\n",
    "    y: pd.DataFrame = data[[\"quality\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return DataSet(X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:47:05.640511Z",
     "start_time": "2023-08-07T20:47:05.545921Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training Function\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "\n",
    "class HyperParameters(BaseModel):\n",
    "    n_estimators: int\n",
    "    max_depth: int\n",
    "    reg_lambda: float\n",
    "    gamma: float\n",
    "    early_stopping_rounds: int\n",
    "\n",
    "\n",
    "def train(ds: DataSet, parameters: HyperParameters) -> str:\n",
    "    # Start the MLflow run to track the model training.\n",
    "    with mlflow.start_run(run_name=create_unique_name(name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"])) as run:\n",
    "        # Enable MLflow logging\n",
    "        mlflow.xgboost.autolog()\n",
    "\n",
    "        # https://xgboost.readthedocs.io/en/stable/python/python_api.html\n",
    "        regressor = xgb.XGBRegressor(\n",
    "            n_estimators=parameters.n_estimators,\n",
    "            max_depth=parameters.max_depth,\n",
    "            reg_lambda=parameters.reg_lambda,\n",
    "            gamma=parameters.gamma,\n",
    "            early_stopping_rounds=parameters.early_stopping_rounds,\n",
    "        )\n",
    "        regressor.fit(X=ds.X_train, y=ds.y_train, eval_set=[(ds.X_test, ds.y_test)], verbose=False)\n",
    "\n",
    "        # Return the run_id for training run comparisons.\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-08-07T20:47:24.818803Z",
     "start_time": "2023-08-07T20:47:06.927642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 358ed7cb7ceb4806a2846a79592a5221\n",
      "{'stopped_iteration': 17.0, 'best_iteration': 17.0, 'validation_0-rmse': 0.6415873227145958}\n"
     ]
    }
   ],
   "source": [
    "from mlflow.entities import Run\n",
    "\n",
    "data_set: DataSet = prepare_data(csv_url=\"datasets/wine-quality.csv\")\n",
    "parameters = HyperParameters(n_estimators=18, max_depth=10, reg_lambda=1, gamma=0, early_stopping_rounds=10)\n",
    "\n",
    "run_id: str = train(ds=data_set, parameters=parameters)\n",
    "stand_alone_run: Run = client.search_runs([experiment.experiment_id], f\"attributes.run_id = '{run_id}'\")[0]\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(stand_alone_run.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Determine the performance on the `Production` Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = 'models:/demo_sklearn_elasticnet_wine/Production'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model, suppress_warnings=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T20:53:00.455699Z",
     "start_time": "2023-08-07T20:53:00.329530Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00        13\n",
      "           4       0.00      0.00      0.00       131\n",
      "           5       0.54      0.26      0.35      1099\n",
      "           6       0.49      0.87      0.63      1654\n",
      "           7       0.46      0.14      0.22       639\n",
      "           8       0.00      0.00      0.00       132\n",
      "           9       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.49      3673\n",
      "   macro avg       0.21      0.18      0.17      3673\n",
      "weighted avg       0.46      0.49      0.42      3673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Performance on Training data\n",
    "y_pred: pd.DataFrame = pd.DataFrame(loaded_model.predict(data_set.X_train), columns=[\"quality\"])\n",
    "y_pred[\"quality\"] = y_pred[\"quality\"].round().astype(dtype=int)\n",
    "print(classification_report(y_true=data_set.y_train, y_pred=y_pred, labels=[3,4,5,6,7,8,9]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:07:47.458729Z",
     "start_time": "2023-08-07T21:07:47.437892Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00        32\n",
      "           5       0.47      0.23      0.31       358\n",
      "           6       0.48      0.86      0.62       544\n",
      "           7       0.55      0.17      0.26       241\n",
      "           8       0.00      0.00      0.00        43\n",
      "           9       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.48      0.48      0.48      1225\n",
      "   macro avg       0.21      0.18      0.17      1225\n",
      "weighted avg       0.46      0.48      0.42      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Performance on Test data\n",
    "y_pred: pd.DataFrame = pd.DataFrame(loaded_model.predict(data_set.X_test), columns=[\"quality\"])\n",
    "y_pred[\"quality\"] = y_pred[\"quality\"].round().astype(dtype=int)\n",
    "print(classification_report(y_true=data_set.y_test, y_pred=y_pred, labels=[3,4,5,6,7,8,9]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-07T21:07:48.460777Z",
     "start_time": "2023-08-07T21:07:48.449808Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a naive search of the hyperparameter space\n",
    "\n",
    "We will naively review model performance at specific internals across the solution space.  There are many optimization functions which can be leveraged base on business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "runs: list[str] = []\n",
    "\n",
    "for i in trange(3, 9):\n",
    "    n_estimators: int = i * 2 + 1\n",
    "    for j in range(3, 9):\n",
    "        max_depth: int = j + 3\n",
    "        data_set: DataSet = prepare_data(csv_url=\"datasets/wine-quality.csv\")\n",
    "        parameters = HyperParameters(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            reg_lambda=1,\n",
    "            gamma=0,\n",
    "            early_stopping_rounds=10,\n",
    "        )\n",
    "        run_id: str = train(ds=data_set, parameters=parameters)\n",
    "        runs.append(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and register the best model\n",
    "Define our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Run\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def get_best_run(client: MlflowClient, experiment_id, runs: list[str]) -> tuple[Optional[Run], dict]:\n",
    "    _inf = np.finfo(np.float64).max\n",
    "\n",
    "    best_metrics: dict = {\n",
    "        \"validation_0-rmse\": _inf,\n",
    "    }\n",
    "    best_run: Optional[Run] = None\n",
    "\n",
    "    for run_id in runs:\n",
    "        # find the best run, log its metrics as the final metrics of this run.\n",
    "        run: Run = client.search_runs([experiment_id], f\"attributes.run_id = '{run_id}'\")[0]\n",
    "        if (\n",
    "            \"validation_0-rmse\" in run.data.metrics\n",
    "            and run.data.metrics[\"validation_0-rmse\"] < best_metrics[\"validation_0-rmse\"]\n",
    "        ):\n",
    "            best_metrics = run.data.metrics\n",
    "            best_run = run\n",
    "\n",
    "    return best_run, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.entities.model_registry import ModelVersion\n",
    "\n",
    "\n",
    "def register_best_model(client: MlflowClient, run: Run) -> ModelVersion:\n",
    "    model_version: ModelVersion = client.create_model_version(\n",
    "        name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
    "        source=f\"{run.info.artifact_uri}/model\",\n",
    "        run_id=run.info.run_id,\n",
    "        tags={\"run_id\": run.info.run_id},\n",
    "    )\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the runs for the best performing model and add it to the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(best_run, metrics) = get_best_run(client=client, experiment_id=experiment.experiment_id, runs=runs)\n",
    "model_version: ModelVersion = register_best_model(client=client, run=best_run)\n",
    "\n",
    "print(f\"Run ID: {best_run.info.run_id}\")\n",
    "print(f\"Report: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promote the latest model to the `Production` stage for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version: ModelVersion = client.transition_model_version_stage(\n",
    "    name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
    "    version=model_version.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
