{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Training Demo\n",
    "\n",
    "Creates a model for predicting the quality of wine using [sklearn.linear_model.ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html).  We perform a naive search of the hyper-parameter space in order to determine the optimal values.\n",
    "\n",
    "The results of the model training runs are tracked in an MLflow experiment. The best performing model is then registered in the model registry and set to the `Production` stage for usage.\n",
    "\n",
    "> This is notebook is based on `train.py` from the MLflow example [sklearn_elasticnet_wine](https://github.com/mlflow/mlflow/tree/master/examples/sklearn_elasticnet_wine).\n",
    "\n",
    "Attribution\n",
    "* The data set used in this example is from http://archive.ics.uci.edu/ml/datasets/Wine+Quality and sourced from [here](https://raw.githubusercontent.com/mlflow/mlflow/master/examples/sklearn_elasticnet_wine/wine-quality.csv).\n",
    "* P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "* Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from mlflow.models.signature import infer_signature\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "\n",
    "from mlflow_adsp import create_unique_name, upsert_experiment\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load user specific configuration.\n",
    "\"\"\"\n",
    "\n",
    "from ae5_tools import load_ae5_user_secrets\n",
    "\n",
    "load_ae5_user_secrets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Setup\n",
    "\n",
    "Create our experiment to track all our model training runs in.\n",
    "\n",
    "* This experiment is used across runs of the notebook and will not be recreated if it already exists.\n",
    "* The name of the experiment is defined as an anaconda project variable located within `anaconda-project.yml`.\n",
    "    * The variable name is `MLFLOW_EXPERIMENT_NAME`, and the default value is `demo_sklearn_elasticnet_wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Generate a client, this will be used for several operations across the notebook.\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "\n",
    "def upsert_model_registry(client: MlflowClient) -> None:\n",
    "    try:\n",
    "        client.create_registered_model(\n",
    "            name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"]\n",
    "        )\n",
    "    except MlflowException as error:\n",
    "        if error.error_code != \"RESOURCE_ALREADY_EXISTS\":\n",
    "            raise error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ensure that the experiment and model registry exist for reporting and tracking.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from mlflow.entities import Experiment\n",
    "\n",
    "experiment: Experiment = mlflow.set_experiment(\n",
    "    experiment_id=upsert_experiment()\n",
    ")\n",
    "upsert_model_registry(client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Preparation\n",
    "Loads the data from csv file, and returns our train, test splits for training.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def prepare_data(csv_url: str) -> dict:\n",
    "    data: pd.DataFrame = pd.read_csv(csv_url, sep=\",\")\n",
    "\n",
    "    # The predicted column is \"quality\" which is a scalar from [3, 9]\n",
    "    X: pd.DataFrame = data.drop([\"quality\"], axis=1)\n",
    "    y: pd.DataFrame = data[[\"quality\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return {\"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training performance evaluation function\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Training Function\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from utils import fetch_logged_data\n",
    "\n",
    "\n",
    "def train(training_data: dict) -> str:\n",
    "    X_train = training_data[\"X_train\"]\n",
    "    X_test = training_data[\"X_test\"]\n",
    "    y_train = training_data[\"y_train\"]\n",
    "    y_test = training_data[\"y_test\"]\n",
    "\n",
    "    # Start the MLflow run to track the model training.\n",
    "    with mlflow.start_run(\n",
    "        run_name=create_unique_name(name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"])\n",
    "    ) as run:\n",
    "        # enable auto logging\n",
    "        # this includes xgboost.sklearn estimators\n",
    "        mlflow.xgboost.autolog()\n",
    "\n",
    "        regressor = xgb.XGBRegressor(n_estimators=20, reg_lambda=1, gamma=0, max_depth=3)\n",
    "        regressor.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "        y_pred = regressor.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        run_id = mlflow.last_active_run().info.run_id\n",
    "        print(\"Logged data and model in run {}\".format(run_id))\n",
    "\n",
    "        # show logged data\n",
    "        for key, data in fetch_logged_data(run_id).items():\n",
    "            print(\"\\n---------- logged {} ----------\".format(key))\n",
    "            pprint(data)\n",
    "\n",
    "        # Return the run_id for training run comparisons.\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.entities import Run\n",
    "\n",
    "training_data: dict = prepare_data(csv_url=\"datasets/wine-quality.csv\")\n",
    "\n",
    "run_id: str = train(training_data=training_data)\n",
    "run: Run = client.search_runs(\n",
    "    [experiment.experiment_id], f\"attributes.run_id = '{run_id}'\"\n",
    ")[0]\n",
    "print(run.data.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a naive search of the hyperparameter space\n",
    "\n",
    "We will naively review model performance at specific internals across the solution space.  There are many optimization functions which can be leveraged base on business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm import trange\n",
    "#\n",
    "# runs: list[str] = []\n",
    "#\n",
    "# for i in trange(5):\n",
    "#     alpha: float = i * 0.1\n",
    "#     for j in trange(5, leave=False):\n",
    "#         l1_ratio: float = j * 0.1\n",
    "#         run_id: str = train(\n",
    "#             alpha=alpha,\n",
    "#             l1_ratio=l1_ratio,\n",
    "#             training_data=prepare_data(csv_url=\"datasets/wine-quality.csv\"),\n",
    "#         )\n",
    "#         runs.append(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and register the best model\n",
    "Define our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Run\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def get_best_run(\n",
    "    client: MlflowClient, experiment_id, runs: list[str]\n",
    ") -> tuple[Optional[Run], dict]:\n",
    "    _inf = np.finfo(np.float64).max\n",
    "\n",
    "    best_metrics: dict = {\n",
    "        \"rmse\": _inf,\n",
    "        \"mae\": _inf,\n",
    "        \"r2\": _inf,\n",
    "    }\n",
    "    best_run: Optional[Run] = None\n",
    "\n",
    "    for run_id in runs:\n",
    "        # find the best run, log its metrics as the final metrics of this run.\n",
    "        run: Run = client.search_runs(\n",
    "            [experiment_id], f\"attributes.run_id = '{run_id}'\"\n",
    "        )[0]\n",
    "\n",
    "        if run.data.metrics[\"rmse\"] < best_metrics[\"rmse\"]:\n",
    "            best_metrics = run.data.metrics\n",
    "            best_run = run\n",
    "\n",
    "    return best_run, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.entities.model_registry import ModelVersion\n",
    "\n",
    "\n",
    "def register_best_model(client: MlflowClient, run: Run) -> ModelVersion:\n",
    "    model_version: ModelVersion = client.create_model_version(\n",
    "        name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
    "        source=f\"{run.info.artifact_uri}/model\",\n",
    "        run_id=run.info.run_id,\n",
    "        tags={\"run_id\": run.info.run_id},\n",
    "    )\n",
    "    return model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the runs for the best performing model and add it to the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version: ModelVersion = register_best_model(client=client, run=run)\n",
    "(run, metrics) = get_best_run(\n",
    "    client=client, experiment_id=experiment.experiment_id, runs=runs\n",
    ")\n",
    "\n",
    "print(f\"Run ID: {run.info.run_id}\")\n",
    "print(f\"Report: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promote the latest model to the `Production` stage for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version: ModelVersion = client.transition_model_version_stage(\n",
    "    name=os.environ[\"MLFLOW_EXPERIMENT_NAME\"],\n",
    "    version=model_version.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
